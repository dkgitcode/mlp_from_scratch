{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_data(X_train, X_test, y_train, y_test):\n",
    "    # Combine train and test set to ensure same dummy variables\n",
    "    X_combined = pd.concat([X_train, X_test])\n",
    "    \n",
    "    # Apply get_dummies to the combined dataset\n",
    "    X_combined = pd.get_dummies(X_combined, drop_first=True)\n",
    "    \n",
    "    # Split the combined dataset back into train and test sets\n",
    "    X_train = X_combined.iloc[:len(X_train)]\n",
    "    X_test = X_combined.iloc[len(X_train):]\n",
    "    \n",
    "    # Z-score normalization\n",
    "    X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "    X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "\n",
    "    # Handle potential NaN values resulting from normalization\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "    # Convert categorical target variable y to binary (0/1)\n",
    "    y_train = (y_train == '>50K').astype(int)\n",
    "    y_test = (y_test == '>50K').astype(int)\n",
    "\n",
    "    # Convert data to np arrays\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of adults making over 50k: [6279]\n",
      "Number of adults making under 50k: [32794]\n"
     ]
    }
   ],
   "source": [
    "# how many of y_train are classified as 1\n",
    "adults_over_50k = sum(y_train)\n",
    "# how many of y_Train are classified as 0 \n",
    "adults_under_50k = len(y_train) - adults_over_50k\n",
    "\n",
    "print(f\"Number of adults making over 50k: {adults_over_50k}\")\n",
    "print(f\"Number of adults making under 50k: {adults_under_50k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39073, 100), (9769, 100), (39073, 1), (9769, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, n_inputs, n_neurons, activation_function=None, activation_prime=None):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) # Initialize weights randomly from -.5 to .5\n",
    "        self.biases = np.zeros((1, n_neurons)) # Initialize biases to 0\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_prime = activation_prime \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculates the layer's output for a given input.\n",
    "        :param inputs: The input to the layer.\n",
    "        :return: The activated output of the layer.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = (\n",
    "            self.z\n",
    "            if self.activation_function is None\n",
    "            else self.activation_function(self.z)\n",
    "        )\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, dL_dA):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the loss with respect to weights, biases, and the previous layer's activations.\n",
    "        :param dL_dA: The gradient of the loss with respect to the layer's output.\n",
    "        :return dL_dA_prev: The gradient of the loss with respect to the previous layer's activations.\n",
    "        \"\"\"\n",
    "        if self.activation_function is None:\n",
    "            self.dL_dz = dL_dA\n",
    "        else:\n",
    "            self.dL_dz = dL_dA * self.activation_prime(self.z)\n",
    "        # Remember, z = w * a + b, so dz/dw = a\n",
    "        self.dL_dW = np.dot(self.inputs.T, self.dL_dz) # dL/dw = dL/da * da/dz * dz/dw\n",
    "        self.dL_dB = np.sum(self.dL_dz, axis=0, keepdims=True) # dL/db = dL/da * da/dz * dz/db\n",
    "        \n",
    "        self.dL_dA_prev = np.dot(self.dL_dz, self.weights.T) # dL/dz * dz/dA[L-1]\n",
    "        \n",
    "        return self.dL_dA_prev\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of the layer by moving in the opposite direction of the stored gradients.\n",
    "        :param learning_rate: The size of the step to take.\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * self.dL_dW\n",
    "        self.biases -= learning_rate * self.dL_dB\n",
    "        \n",
    "    def clip_gradients(self, max_norm):\n",
    "        \"\"\"\n",
    "        Clips the gradients to prevent exploding gradients using L2 norm clipping.\n",
    "        :param max_norm: The maximum allowable norm for the gradients.\n",
    "        \"\"\"\n",
    "        total_norm = np.linalg.norm(self.dL_dW) # Calculate the L2 norm of the gradients\n",
    "        if total_norm > max_norm:\n",
    "            self.dL_dW = self.dL_dW * (max_norm / total_norm) # Rescale the gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danjk\\AppData\\Local\\Temp\\ipykernel_27404\\3952117944.py:41: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.678217000074567\n",
      "Epoch: 1, Loss: 1.5586221620675547\n",
      "Epoch: 2, Loss: 0.47156914718768017\n",
      "Epoch: 3, Loss: 0.3982361344784691\n",
      "Epoch: 4, Loss: 0.37312888737075406\n",
      "Epoch: 5, Loss: 0.3585898627314312\n",
      "Epoch: 6, Loss: 0.34221082344148146\n",
      "Epoch: 7, Loss: 0.3317781559526788\n",
      "Epoch: 8, Loss: 0.32625520797964036\n",
      "Epoch: 9, Loss: 0.32213068325532757\n",
      "Epoch: 10, Loss: 0.3184936686820696\n",
      "Epoch: 11, Loss: 0.31531577031425\n",
      "Epoch: 12, Loss: 0.3125699771062825\n",
      "Epoch: 13, Loss: 0.3102219053066221\n",
      "Epoch: 14, Loss: 0.3078886513885265\n",
      "Epoch: 15, Loss: 0.30622102830618414\n",
      "Epoch: 16, Loss: 0.30426073582177293\n",
      "Epoch: 17, Loss: 0.3026634291600917\n",
      "Epoch: 18, Loss: 0.3012468732021221\n",
      "Epoch: 19, Loss: 0.3002182275538207\n",
      "Epoch: 20, Loss: 0.29947395703003576\n",
      "Epoch: 21, Loss: 0.2984477045580124\n",
      "Epoch: 22, Loss: 0.2972821025609699\n",
      "Epoch: 23, Loss: 0.29654885591768676\n",
      "Epoch: 24, Loss: 0.29570166342497\n",
      "Epoch: 25, Loss: 0.29525345915611473\n",
      "Epoch: 26, Loss: 0.29500784669629543\n",
      "Epoch: 27, Loss: 0.2944404287762891\n",
      "Epoch: 28, Loss: 0.29384069124454054\n",
      "Epoch: 29, Loss: 0.29324783743674165\n",
      "Epoch: 30, Loss: 0.2925558998732202\n",
      "Epoch: 31, Loss: 0.29197670672572257\n",
      "Epoch: 32, Loss: 0.29187267696337515\n",
      "Epoch: 33, Loss: 0.2915890672545191\n",
      "Epoch: 34, Loss: 0.29132035718474353\n",
      "Epoch: 35, Loss: 0.2908148224929362\n",
      "Epoch: 36, Loss: 0.290296262256365\n",
      "Epoch: 37, Loss: 0.28983816300654125\n",
      "Epoch: 38, Loss: 0.28954199426147936\n",
      "Epoch: 39, Loss: 0.28916375551436274\n",
      "Epoch: 40, Loss: 0.28878363934543494\n",
      "Epoch: 41, Loss: 0.2885783878429541\n",
      "Epoch: 42, Loss: 0.2880160506977027\n",
      "Epoch: 43, Loss: 0.28772351911520216\n",
      "Epoch: 44, Loss: 0.2871021759742117\n",
      "Epoch: 45, Loss: 0.2865544951780013\n",
      "Epoch: 46, Loss: 0.2859801514301161\n",
      "Epoch: 47, Loss: 0.2856051388504746\n",
      "Epoch: 48, Loss: 0.2851178346045409\n",
      "Epoch: 49, Loss: 0.28472778151782935\n",
      "Epoch: 50, Loss: 0.2844197919462322\n",
      "Epoch: 51, Loss: 0.2838368233380313\n",
      "Epoch: 52, Loss: 0.2835675717792269\n",
      "Epoch: 53, Loss: 0.2831370912410078\n",
      "Epoch: 54, Loss: 0.28258318835615975\n",
      "Epoch: 55, Loss: 0.281871857191437\n",
      "Epoch: 56, Loss: 0.28130536058477495\n",
      "Epoch: 57, Loss: 0.28084667947266173\n",
      "Epoch: 58, Loss: 0.28039168371273154\n",
      "Epoch: 59, Loss: 0.2799694002716231\n",
      "Epoch: 60, Loss: 0.2794295099942064\n",
      "Epoch: 61, Loss: 0.2790330494872086\n",
      "Epoch: 62, Loss: 0.2787725115071125\n",
      "Epoch: 63, Loss: 0.2781914047394124\n",
      "Epoch: 64, Loss: 0.27787370561241126\n",
      "Epoch: 65, Loss: 0.27752193386495355\n",
      "Epoch: 66, Loss: 0.27729778461940613\n",
      "Epoch: 67, Loss: 0.2770922015463362\n",
      "Epoch: 68, Loss: 0.2766460244036172\n",
      "Epoch: 69, Loss: 0.2763465027602226\n",
      "Epoch: 70, Loss: 0.27602975120915896\n",
      "Epoch: 71, Loss: 0.2757064723059504\n",
      "Epoch: 72, Loss: 0.275573351034448\n",
      "Epoch: 73, Loss: 0.27524082894696766\n",
      "Epoch: 74, Loss: 0.27499082217183723\n",
      "Epoch: 75, Loss: 0.27473676282958825\n",
      "Epoch: 76, Loss: 0.27447554889641046\n",
      "Epoch: 77, Loss: 0.2743964354304582\n",
      "Epoch: 78, Loss: 0.2741423190479422\n",
      "Epoch: 79, Loss: 0.27394494770676375\n",
      "Epoch: 80, Loss: 0.27391581430119993\n",
      "Epoch: 81, Loss: 0.273663766722404\n",
      "Epoch: 82, Loss: 0.27337318131872423\n",
      "Epoch: 83, Loss: 0.2732441276004131\n",
      "Epoch: 84, Loss: 0.2730504347724875\n",
      "Epoch: 85, Loss: 0.27306430171172213\n",
      "Epoch: 86, Loss: 0.2727578242505435\n",
      "Epoch: 87, Loss: 0.2727181708801006\n",
      "Epoch: 88, Loss: 0.2724854299912444\n",
      "Epoch: 89, Loss: 0.27228883495289374\n",
      "Epoch: 90, Loss: 0.27183525906893263\n",
      "Epoch: 91, Loss: 0.27187325117951666\n",
      "Epoch: 92, Loss: 0.2714768831896557\n",
      "Epoch: 93, Loss: 0.2713548114167299\n",
      "Epoch: 94, Loss: 0.27107316709413015\n",
      "Epoch: 95, Loss: 0.27094699103774994\n",
      "Epoch: 96, Loss: 0.270717290762386\n",
      "Epoch: 97, Loss: 0.2704926617602978\n",
      "Epoch: 98, Loss: 0.2703048087909773\n",
      "Epoch: 99, Loss: 0.27011613184683575\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer(X_train.shape[1], 64, relu, relu_prime)\n",
    "layer2 = Layer(64, 32, relu, relu_prime)\n",
    "layer3 = Layer(32, 1, sigmoid, sigmoid_prime)\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "epsilon = 1e-15\n",
    "l2_lambda = 0.01\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    learning_rate = initial_learning_rate * (1 / (1 + 0.01 * epoch))  # Learning rate decay\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        A1 = layer1.forward(X_batch)\n",
    "        A2 = layer2.forward(A1)\n",
    "        A3 = layer3.forward(A2)\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        A3 = np.clip(A3, epsilon, 1 - epsilon) # Ensure that A3 is > 0 and < 1 but not exactly 0 or 1\n",
    "        loss = -np.mean(y_batch * np.log(A3) + (1 - y_batch) * np.log(1 - A3)) # Calculate loss for tracking\n",
    "        total_loss += loss\n",
    "        \n",
    "        dL_dA3 = -(y_batch / A3) + (1 - y_batch) / (1 - A3) \n",
    "        \n",
    "        # Backward pass\n",
    "        dL_dA2 = layer3.backward(dL_dA3)\n",
    "        dL_dA1 = layer2.backward(dL_dA2)\n",
    "        dL_dA0 = layer1.backward(dL_dA1)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for layer in [layer1, layer2, layer3]:\n",
    "            layer.clip_gradients(5.0) # Range of the gradients that we are allowing\n",
    "            layer.update(learning_rate) \n",
    "            layer.weights -= learning_rate * l2_lambda * layer.weights  # L2 regularization\n",
    "    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {total_loss / (len(X_train) / batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danjk\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3438: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "income    0.839595\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass on test set\n",
    "A1 = layer1.forward(X_test)\n",
    "A2 = layer2.forward(A1)\n",
    "A3 = layer3.forward(A2)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (A3 > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
