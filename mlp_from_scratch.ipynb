{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(X_train, X_test, y_train, y_test):\n",
    "    # Combine train and test set to ensure same dummy variables\n",
    "    X_combined = pd.concat([X_train, X_test])\n",
    "    \n",
    "    # Apply get_dummies to the combined dataset\n",
    "    X_combined = pd.get_dummies(X_combined, drop_first=True)\n",
    "    \n",
    "    # Split the combined dataset back into train and test sets\n",
    "    X_train = X_combined.iloc[:len(X_train)]\n",
    "    X_test = X_combined.iloc[len(X_train):]\n",
    "    \n",
    "    # Z-score normalization\n",
    "    X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "    X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "\n",
    "    # Handle potential NaN values resulting from normalization\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "    # Convert categorical target variable y to binary (0/1)\n",
    "    y_train = (y_train == '>50K').astype(int)\n",
    "    y_test = (y_test == '>50K').astype(int)\n",
    "\n",
    "    # Convert data to np arrays\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39073, 100), (9769, 100), (39073, 1), (9769, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip values to avoid overflow\n",
    "    return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, n_inputs, n_neurons, activation_function=None, activation_prime=None):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) # Initialize weights randomly from -.5 to .5\n",
    "        self.biases = np.zeros((1, n_neurons)) # Initialize biases to 0\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_prime = activation_prime \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculates the layer's output for a given input.\n",
    "        :param inputs: The input to the layer.\n",
    "        :return: The activated output of the layer.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = (\n",
    "            self.z\n",
    "            if self.activation_function is None\n",
    "            else self.activation_function(self.z)\n",
    "        )\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, dL_dA):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the loss with respect to weights, biases, and the previous layer's activations.\n",
    "        :param dL_dA: The gradient of the loss with respect to the layer's output.\n",
    "        :return dL_dA_prev: The gradient of the loss with respect to the previous layer's activations.\n",
    "        \"\"\"\n",
    "        if self.activation_function is None:\n",
    "            self.dL_dz = dL_dA\n",
    "        else:\n",
    "            self.dL_dz = dL_dA * self.activation_prime(self.z)\n",
    "        # Remember, z = w * a + b, so dz/dw = a\n",
    "        self.dL_dW = np.dot(self.inputs.T, self.dL_dz) # dL/dw = dL/da * da/dz * dz/dw\n",
    "        self.dL_dB = np.sum(self.dL_dz, axis=0, keepdims=True) # dL/db = dL/da * da/dz * dz/db\n",
    "        \n",
    "        self.dL_dA_prev = np.dot(self.dL_dz, self.weights.T) # dL/dz * dz/dA[L-1]\n",
    "        \n",
    "        return self.dL_dA_prev\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of the layer by moving in the opposite direction of the stored gradients.\n",
    "        :param learning_rate: The size of the step to take.\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * self.dL_dW\n",
    "        self.biases -= learning_rate * self.dL_dB\n",
    "        \n",
    "    def clip_gradients(self, max_norm):\n",
    "        \"\"\"\n",
    "        Clips the gradients to prevent exploding gradients using L2 norm clipping.\n",
    "        :param max_norm: The maximum allowable norm for the gradients.\n",
    "        \"\"\"\n",
    "        total_norm = np.linalg.norm(self.dL_dW) # Calculate the L2 norm of the gradients\n",
    "        if total_norm > max_norm:\n",
    "            self.dL_dW = self.dL_dW * (max_norm / total_norm) # Rescale the gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.618106174150395\n",
      "Epoch: 1, Loss: 3.4751024367844336\n",
      "Epoch: 2, Loss: 0.6538478477019336\n",
      "Epoch: 3, Loss: 0.44810802227233315\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer(X_train.shape[1], 64, relu, relu_prime)\n",
    "layer2 = Layer(64, 32, relu, relu_prime)\n",
    "layer3 = Layer(32, 1, sigmoid, sigmoid_prime)\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "epsilon = 1e-15\n",
    "l2_lambda = 0.01\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    learning_rate = initial_learning_rate * (1 / (1 + 0.01 * epoch))  # Learning rate decay\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        A1 = layer1.forward(X_batch)\n",
    "        A2 = layer2.forward(A1)\n",
    "        A3 = layer3.forward(A2)\n",
    "        \n",
    "        \n",
    "        A3 = np.clip(A3, epsilon, 1 - epsilon) # Ensure that A3 is > 0 and < 1 but not exactly 0 or 1\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        loss = -np.mean(y_batch * np.log(A3) + (1 - y_batch) * np.log(1 - A3)) # Calculate loss for tracking\n",
    "        total_loss += loss\n",
    "                \n",
    "        # Backward pass\n",
    "        dL_dA3 = -(y_batch / A3) + (1 - y_batch) / (1 - A3) \n",
    "        dL_dA2 = layer3.backward(dL_dA3)\n",
    "        dL_dA1 = layer2.backward(dL_dA2)\n",
    "        dL_dA0 = layer1.backward(dL_dA1)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for layer in [layer1, layer2, layer3]:\n",
    "            layer.clip_gradients(5.0) # Range of the gradients that we are allowing\n",
    "            layer.update(learning_rate) \n",
    "            layer.weights -= learning_rate * l2_lambda * layer.weights  # L2 regularization\n",
    "    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {total_loss / (len(X_train) / batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8439963148735797"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass on test set\n",
    "A1 = layer1.forward(X_test)\n",
    "A2 = layer2.forward(A1)\n",
    "A3 = layer3.forward(A2)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (A3 > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
