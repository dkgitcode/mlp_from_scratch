{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(X_train, X_test, y_train, y_test):\n",
    "    # Combine train and test set to ensure same dummy variables\n",
    "    X_combined = pd.concat([X_train, X_test])\n",
    "    \n",
    "    # Apply get_dummies to the combined dataset\n",
    "    X_combined = pd.get_dummies(X_combined, drop_first=True)\n",
    "    \n",
    "    # Split the combined dataset back into train and test sets\n",
    "    X_train = X_combined.iloc[:len(X_train)]\n",
    "    X_test = X_combined.iloc[len(X_train):]\n",
    "    \n",
    "    # Z-score normalization\n",
    "    X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "    X_test = (X_test - X_test.mean()) / X_test.std()\n",
    "\n",
    "    # Handle potential NaN values resulting from normalization\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "    # Convert categorical target variable y to binary (0/1)\n",
    "    y_train = (y_train == '>50K').astype(int)\n",
    "    y_test = (y_test == '>50K').astype(int)\n",
    "\n",
    "    # Convert data to np arrays\n",
    "    X_train = X_train.values\n",
    "    X_test = X_test.values\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prep_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of adults making over 50k: [6279]\n",
      "Number of adults making under 50k: [32794]\n"
     ]
    }
   ],
   "source": [
    "# how many of y_train are classified as 1\n",
    "adults_over_50k = sum(y_train)\n",
    "# how many of y_Train are classified as 0 \n",
    "adults_under_50k = len(y_train) - adults_over_50k\n",
    "\n",
    "print(f\"Number of adults making over 50k: {adults_over_50k}\")\n",
    "print(f\"Number of adults making under 50k: {adults_under_50k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39073, 100), (9769, 100), (39073, 1), (9769, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       income\n",
       "0       <=50K\n",
       "1       <=50K\n",
       "2       <=50K\n",
       "3       <=50K\n",
       "4       <=50K\n",
       "...       ...\n",
       "48837  <=50K.\n",
       "48838  <=50K.\n",
       "48839  <=50K.\n",
       "48840  <=50K.\n",
       "48841   >50K.\n",
       "\n",
       "[48842 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip values to avoid overflow\n",
    "    return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "class Layer():\n",
    "    def __init__(self, n_inputs, n_neurons, activation_function=None, activation_prime=None):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) # Initialize weights randomly from -.5 to .5\n",
    "        self.biases = np.zeros((1, n_neurons)) # Initialize biases to 0\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_prime = activation_prime \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculates the layer's output for a given input.\n",
    "        :param inputs: The input to the layer.\n",
    "        :return: The activated output of the layer.\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = (\n",
    "            self.z\n",
    "            if self.activation_function is None\n",
    "            else self.activation_function(self.z)\n",
    "        )\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self, dL_dA):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the loss with respect to weights, biases, and the previous layer's activations.\n",
    "        :param dL_dA: The gradient of the loss with respect to the layer's output.\n",
    "        :return dL_dA_prev: The gradient of the loss with respect to the previous layer's activations.\n",
    "        \"\"\"\n",
    "        if self.activation_function is None:\n",
    "            self.dL_dz = dL_dA\n",
    "        else:\n",
    "            self.dL_dz = dL_dA * self.activation_prime(self.z)\n",
    "        # Remember, z = w * a + b, so dz/dw = a\n",
    "        self.dL_dW = np.dot(self.inputs.T, self.dL_dz) # dL/dw = dL/da * da/dz * dz/dw\n",
    "        self.dL_dB = np.sum(self.dL_dz, axis=0, keepdims=True) # dL/db = dL/da * da/dz * dz/db\n",
    "        \n",
    "        self.dL_dA_prev = np.dot(self.dL_dz, self.weights.T) # dL/dz * dz/dA[L-1]\n",
    "        \n",
    "        return self.dL_dA_prev\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of the layer by moving in the opposite direction of the stored gradients.\n",
    "        :param learning_rate: The size of the step to take.\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * self.dL_dW\n",
    "        self.biases -= learning_rate * self.dL_dB\n",
    "        \n",
    "    def clip_gradients(self, max_norm):\n",
    "        \"\"\"\n",
    "        Clips the gradients to prevent exploding gradients using L2 norm clipping.\n",
    "        :param max_norm: The maximum allowable norm for the gradients.\n",
    "        \"\"\"\n",
    "        total_norm = np.linalg.norm(self.dL_dW) # Calculate the L2 norm of the gradients\n",
    "        if total_norm > max_norm:\n",
    "            self.dL_dW = self.dL_dW * (max_norm / total_norm) # Rescale the gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.0108087120259865\n",
      "Epoch: 10, Loss: 0.31389578413955815\n",
      "Epoch: 20, Loss: 0.3001462009138574\n",
      "Epoch: 30, Loss: 0.2973327554456927\n",
      "Epoch: 40, Loss: 0.29449745124351306\n",
      "Epoch: 50, Loss: 0.2906720342923743\n",
      "Epoch: 60, Loss: 0.28641827368103406\n",
      "Epoch: 70, Loss: 0.2829651524278782\n",
      "Epoch: 80, Loss: 0.2794582015703953\n",
      "Epoch: 90, Loss: 0.27647367656098554\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer(X_train.shape[1], 64, relu, relu_prime)\n",
    "layer2 = Layer(64, 32, relu, relu_prime)\n",
    "layer3 = Layer(32, 1, sigmoid, sigmoid_prime)\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "epsilon = 1e-15\n",
    "l2_lambda = 0.01\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    learning_rate = initial_learning_rate * (1 / (1 + 0.01 * epoch))  # Learning rate decay\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        A1 = layer1.forward(X_batch)\n",
    "        A2 = layer2.forward(A1)\n",
    "        A3 = layer3.forward(A2)\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        A3 = np.clip(A3, epsilon, 1 - epsilon) # Ensure that A3 is > 0 and < 1 but not exactly 0 or 1\n",
    "        loss = -np.mean(y_batch * np.log(A3) + (1 - y_batch) * np.log(1 - A3)) # Calculate loss for tracking\n",
    "        total_loss += loss\n",
    "        \n",
    "        dL_dA3 = -(y_batch / A3) + (1 - y_batch) / (1 - A3) \n",
    "        \n",
    "        # Backward pass\n",
    "        dL_dA2 = layer3.backward(dL_dA3)\n",
    "        dL_dA1 = layer2.backward(dL_dA2)\n",
    "        dL_dA0 = layer1.backward(dL_dA1)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for layer in [layer1, layer2, layer3]:\n",
    "            layer.clip_gradients(5.0) # Range of the gradients that we are allowing\n",
    "            layer.update(learning_rate) \n",
    "            layer.weights -= learning_rate * l2_lambda * layer.weights  # L2 regularization\n",
    "    \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {total_loss / (len(X_train) / batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8362166035418159"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass on test set\n",
    "A1 = layer1.forward(X_test)\n",
    "A2 = layer2.forward(A1)\n",
    "A3 = layer3.forward(A2)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predictions = (A3 > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
